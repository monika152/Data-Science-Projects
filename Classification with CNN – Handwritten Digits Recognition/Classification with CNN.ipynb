{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "## Classification with CNN â€“ Handwritten Digits Recognition (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras, tensorflow\n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CNN \n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paramenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_row, img_col = 28, 28\n",
    "\n",
    "INPUT_SHAPE = (img_row, img_col, 1) # 1x for 1x Layer of grey scaled\n",
    "NUM_CLASS = 10 # 0 bis 9 \n",
    "EPOCHS = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "x_train = x_train.reshape(x_train.shape[0], img_row, img_col, 1)  # i need 2D not 1D\n",
    "x_test = x_test.reshape(x_test.shape[0], img_row, img_col, 1)\n",
    "\n",
    "# Data Type\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "\n",
    "# Scaling the grey values from 0-255 ---> 0-1\n",
    "x_train /= 255\n",
    "x_test /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot Encoding\n",
    "\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train, NUM_CLASS)\n",
    "y_test = tensorflow.keras.utils.to_categorical(y_test, NUM_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D (24, kernel_size=(5,5), strides = (1,1), padding = \"same\", activation =\"relu\", input_shape = INPUT_SHAPE ))\n",
    "model.add(MaxPooling2D(pool_size=(2,2) , strides = (2,2)  ))\n",
    "\n",
    "model.add(Conv2D(48, kernel_size=(3,3), strides = (1,1), padding = \"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2) , strides = (2,2)  ))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Hidden Layer(s)\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "\n",
    "# Drop Out Layer  --> sets some weights and neuros to zero randomly\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(NUM_CLASS, activation = \"softmax\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile / Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer =RMSprop(), metrics =[ 'accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: list of callback instances\n",
    "# ACHTUNG: the code of the MyCustomCallback is DOWN ...Scroll to there and run the code fist\n",
    "my_callback_list = [EarlyStopping(min_delta= 0.0001, patience = 5),  # early stopping if no improvement for 5 epochs more than 0.0001\n",
    "                   MyCustomCallback(), # uses my custome call back class\n",
    "                   CSVLogger('./training.log'),  # logs the result for each epoch in a csv file\n",
    "                   ModelCheckpoint(filepath='./models/model.{epoch:02d}-{val_loss:.2f}.h5'), # save the model under the name xyz\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Fitting\n",
    "#history = model.fit(x_train, y_train, epochs=EPOCHS, verbose = 0, validation_data = (x_test, y_test))   # validation_data instead of validation_split\n",
    "\n",
    "# With Callback Earlystopping\n",
    "#history = model.fit(x_train, y_train, epochs=EPOCHS, verbose = 0, validation_data = (x_test, y_test), callbacks=[EarlyStopping(min_delta= 0.0001, patience = 5)])   # validation_data instead of validation_split\n",
    "\n",
    "\n",
    "# With a list of Callbacks\n",
    "history = model.fit(x_train, y_train, epochs=EPOCHS, verbose = 0, validation_data = (x_test, y_test), callbacks= my_callback_list)   # validation_data instead of validation_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot The results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Training_acc\", \"Validation_acc\"])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Loss \n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Training_Loss\", \"Validation_Loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Callback\n",
    "- https://keras.io/guides/writing_your_own_callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Starting training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4d899908bfb14b37e3b4de7522c463ed190a0fb2c4787027e26992d27c31972"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
